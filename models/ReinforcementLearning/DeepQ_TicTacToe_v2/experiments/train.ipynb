{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XReCdM8kILUt"
   },
   "source": [
    "#### Optimal Opponent Experiments\n",
    "Author: Yemi Kelani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vNSWFUkwdXc"
   },
   "source": [
    "##### Google Drive Setup (Skip if running locally)\n",
    "\n",
    "> To run this notebook, follow these steps:\n",
    "> 1. Download the latest version of the [repository](https://github.com/yemi-kelani/artificial-intelligence/tree/master).\n",
    "> 2. Upload the repsitory files to your Google Drive account under the path `Projects/artificial-intelligence`.\n",
    "> 3. Open this file (`train.ipynb`) from your Google Drive and run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17125,
     "status": "ok",
     "timestamp": 1730866117884,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "rEqTNnEdl-8u",
    "outputId": "c4d16bbd-fd7d-46b6-8558-e3c3f72eb41a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730866117884,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "3IlDuGfgof5R"
   },
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"drive/MyDrive/Projects/artificial-intelligence/models/ReinforcementLearning/\"\n",
    "PROJECT_PATH = f\"{ROOT_FOLDER}/DeepQ_TicTacToe_v2\"\n",
    "NOTEBOOK_LOCATION = f\"{PROJECT_PATH}/experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5627,
     "status": "ok",
     "timestamp": 1730866353934,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "5dji1umepw8Z"
   },
   "outputs": [],
   "source": [
    "!cp {PROJECT_PATH}/DeepQAgent.py .\n",
    "!cp {PROJECT_PATH}/TicTacToeGame.py .\n",
    "!cp {ROOT_FOLDER}/Utils.py .\n",
    "\n",
    "from DeepQAgent import DeepQAgent\n",
    "from TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n",
    "from Utils import (\n",
    "    train_agent,\n",
    "    test_agent\n",
    ")\n",
    "MODEL_PATH = \"drive/MyDrive/Projects/artificial-intelligence/trained_models/ReinforcementLearning/TicTacToeV2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOHKJqHY22Ga"
   },
   "source": [
    "##### Local Setup (Skip if running remotely)\n",
    "> 1. Run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8BLUQ7N3Ors"
   },
   "outputs": [],
   "source": [
    "from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent\n",
    "from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n",
    "from models.ReinforcementLearning.Utils import (\n",
    "    train_agent,\n",
    "    test_agent\n",
    ")\n",
    "MODEL_PATH = \"../../../../trained_models/ReinforcementLearning/TicTacToeV2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kw4XZB63VyB"
   },
   "source": [
    "##### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1730866380936,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "IpvCCHUX3Ge2",
    "outputId": "4c3b7b11-0c5b-46ce-d567-ddb81d290876"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# DeepQ parameters\n",
    "BATCH_SIZE     = 256\n",
    "NUM_EPISODES   = 10000\n",
    "STATE_SPACE    = 9\n",
    "ACTION_SPACE   = 9\n",
    "HIDDEN_SIZE    = 256\n",
    "EPSILON        = 1.0\n",
    "GAMMA          = 0.90\n",
    "LEARNING_RATE  = 0.001\n",
    "DROPOUT        = 0.15\n",
    "TRAIN_START    = 1000 # =< 2000 (Maxlen of replay memory)\n",
    "NEGATIVE_SLOPE = 0.01\n",
    "\n",
    "# model roots\n",
    "BASELINE = \"TicTacToe-v2-BASELINE\"\n",
    "NAIVE = \"TicTacToe-v2-NAIVE\"\n",
    "AGENT = \"TicTacToe-v2-AGENT\"\n",
    "OPTIMAL = \"TicTacToe-v2-OPTIMAL\"\n",
    "SELF = \"TicTacToe-v2-SELF\"\n",
    "\n",
    "\n",
    "def get_full_model_path(agent_name):\n",
    "  return os.path.join(MODEL_PATH, agent_name + \".pt\")\n",
    "\n",
    "def supply_model(load_if_exists: bool = True, agent_name: str = None):\n",
    "\n",
    "  agent = DeepQAgent(\n",
    "      device         = DEVICE,\n",
    "      epsilon        = EPSILON,\n",
    "      gamma          = GAMMA,\n",
    "      state_space    = STATE_SPACE,\n",
    "      action_space   = ACTION_SPACE,\n",
    "      hidden_size    = HIDDEN_SIZE,\n",
    "      dropout        = DROPOUT,\n",
    "      train_start    = TRAIN_START,\n",
    "      batch_size     = BATCH_SIZE,\n",
    "      negative_slope = NEGATIVE_SLOPE\n",
    "  )\n",
    "\n",
    "  full_model_path = get_full_model_path(agent_name)\n",
    "  if load_if_exists and os.path.exists(full_model_path):\n",
    "    print(\"Loading Model Parameters...\")\n",
    "    agent.load_model(filepath=full_model_path)\n",
    "\n",
    "  optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "  criterion = torch.nn.SmoothL1Loss() # Huber Loss\n",
    "\n",
    "  return agent, optimizer, criterion\n",
    "\n",
    "def compare_to_naive(agent_name: str, num_episodes: int = 10000):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n",
    "\n",
    "def compare_to_optimal(agent_name: str, num_episodes: int = 100):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n",
    "\n",
    "def compare_to_model(agent_name: str, model_name: str, num_episodes: int = 10000):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _enemy, _, _ = supply_model(load_if_exists=True, agent_name=model_name)\n",
    "  _environment = TicTacToeGame(DEVICE, _enemy, OPPONENT_LEVEL.AGENT, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 5852,
     "status": "ok",
     "timestamp": 1730780311679,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "zgX_xdLvxaaE",
    "outputId": "d27812ce-7610-46d3-d9fe-1badef8b5b5c"
   },
   "outputs": [],
   "source": [
    "agent, _, _ = supply_model()\n",
    "agent.save_model(MODEL_PATH, BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68624,
     "status": "ok",
     "timestamp": 1730780380300,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "KgNZkepzxkQ9",
    "outputId": "cdbd3b32-5b3f-4834-c9ec-6770290ea1ab"
   },
   "outputs": [],
   "source": [
    "compare_to_naive(BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1730866365431,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "mdG4YkFxgxP4"
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "TRAIN_START  = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8fX711p3rw_",
    "outputId": "1eff47fb-57b7-4c77-d996-aecbf5ee210f"
   },
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=BASELINE)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = NAIVE,\n",
    "    save_every = 2000,\n",
    "    epsilon_min_value = 0.30,\n",
    "    epsilon_max_value = 0.75,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73153,
     "status": "ok",
     "timestamp": 1730820076141,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "iuAN5rZ1mqUp",
    "outputId": "1f233258-372b-4892-be19-8c33e42e003b"
   },
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-2K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66366,
     "status": "ok",
     "timestamp": 1730820142504,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "91xiRTvm6VO5",
    "outputId": "b8ede35f-0703-4c73-b5d4-45f5df13952b"
   },
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-4K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59804,
     "status": "ok",
     "timestamp": 1730820202304,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "9tqMcYtw6ZQX",
    "outputId": "b84f6410-aef6-449d-d01e-b2fa7d770532"
   },
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-6K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60215,
     "status": "ok",
     "timestamp": 1730846608402,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "C2Ins307fh4I",
    "outputId": "c28ea8f1-ba72-4d46-ba6a-f75b00828299"
   },
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-8K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59729,
     "status": "ok",
     "timestamp": 1730846668128,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "D0rgvKmXfjjg",
    "outputId": "63625ccf-ff89-484b-9ffc-94e8458fad18"
   },
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-10K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69974,
     "status": "ok",
     "timestamp": 1730862456536,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "QpaZBNOG41pT",
    "outputId": "cfe2662d-8ec7-4e15-b281-0c2014b50590"
   },
   "outputs": [],
   "source": [
    "# The NAIVE-10+ model has trained for\n",
    "# 2000 extra episodes to account for\n",
    "# episodes not trained upon before TRAIN start\n",
    "compare_to_naive(f\"{NAIVE}-10K+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohJkRimiJY-g"
   },
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{NAIVE}-12K\")\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{NAIVE}-12K+\",\n",
    "    save_every = 2000,\n",
    "    epsilon_min_value = 0.40,\n",
    "    epsilon_max_value = 0.80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-12K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-14K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-16K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-18K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-20K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-22K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{NAIVE}-22K\")\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{NAIVE}-22K+\",\n",
    "    save_every = 2000,\n",
    "    epsilon_min_value = 0.20,\n",
    "    epsilon_max_value = 0.80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-24K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-26K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-28K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-30K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-32K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(f\"{NAIVE}-32K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{NAIVE}-32K\")\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{NAIVE}-32K+\",\n",
    "    save_every = 2000,\n",
    "    epsilon_min_value = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-34K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-36K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-38K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-40K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-42K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{NAIVE}-42K\")\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    20500,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{NAIVE}-42K+\",\n",
    "    save_every = 2000,\n",
    "    epsilon_min_value = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-44K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-46K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-48K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-48K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-50K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-52K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{NAIVE}-54K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{NAIVE}-54K\")\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = OPTIMAL,\n",
    "    save_every = 2000,\n",
    "    epsilon_min_value = 0.30,\n",
    "    epsilon_max_value = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-2K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-4K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-6K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-8K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-10K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(f\"{OPTIMAL}-10K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{OPTIMAL}-10K\")\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{OPTIMAL}-10K+\",\n",
    "    save_every = 2000,\n",
    "    epsilon_min_value = 0.20,\n",
    "    epsilon_max_value = 0.50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-12K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-14K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-16K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(f\"{OPTIMAL}-16K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{OPTIMAL}-16K\")\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{OPTIMAL}-16K+\",\n",
    "    save_every = 2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-18K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-18K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-20K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-20K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-22K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-22K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{OPTIMAL}-22K\")\n",
    "agent.prep_cosine_anneal(0.5, 1.0, NUM_EPISODES)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{OPTIMAL}-22K+\",\n",
    "    save_every = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{OPTIMAL}-32K\")\n",
    "agent.prep_cosine_anneal(0.1, 1.0, NUM_EPISODES)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{OPTIMAL}-32K+\",\n",
    "    save_every = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{OPTIMAL}-42K\")\n",
    "agent.prep_cosine_anneal(0.1, 1.0, NUM_EPISODES)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{OPTIMAL}-42K+\",\n",
    "    save_every = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=f\"{OPTIMAL}-52K\")\n",
    "agent.prep_cosine_anneal(0.1, 1.0, NUM_EPISODES)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = f\"{OPTIMAL}-52K+\",\n",
    "    save_every = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-50K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-50K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnbY0pxlm6JTS4enhO6PPu",
   "collapsed_sections": [
    "rOHKJqHY22Ga"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
