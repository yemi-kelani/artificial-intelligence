# CLAUDE.md

> **Last Updated**: December 23, 2024  
> **DQN Implementation Status**: âœ… Fully Repaired and Validated  
> **Training Pipeline Status**: âœ… Production Ready  

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## ğŸ“‘ Table of Contents

- [Development Environment Setup](#development-environment-setup)
- [Common Commands](#common-commands)
  - [Running Interactive TicTacToe Game](#running-interactive-tictactoe-game)
  - [Training and Experimentation](#training-and-experimentation)
- [Architecture Overview](#architecture-overview)
  - [Core Components](#core-components)
  - [Project Structure](#project-structure)
  - [Key Design Patterns](#key-design-patterns)
- [Code Quality and Validation](#code-quality-and-validation)
  - [Environment Validation](#environment-validation)
  - [Model Validation](#model-validation)
  - [Code Quality Checks](#code-quality-checks)
  - [Development Workflow Validation](#development-workflow-validation)
- [Troubleshooting and Best Practices](#troubleshooting-and-best-practices)
  - [Common Issues and Solutions](#-common-issues-and-solutions)
  - [Training Best Practices](#-training-best-practices)
  - [Security Notes](#-security-notes)
- [Recent Major Improvements](#recent-major-improvements-december-2024)
- [Important Notes](#important-notes)

## Development Environment Setup

Install the project in development mode:
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -e .
```

## Common Commands

### Running Interactive TicTacToe Game
```bash
# Play against a trained agent (SECURE - eval() vulnerabilities fixed)
python -m models.ReinforcementLearning.DeepQ_TicTacToe_v2.play MODEL_NAME
```

Available trained models in `trained_models/ReinforcementLearning/TicTacToeV2/`:
- `TicTacToe-v2-BASELINE.pt` - Untrained baseline for comparison
- `TicTacToe-v2-NAIVE-*K.pt` - Models trained against random opponent (various checkpoints)
- `TicTacToe-v2-OPTIMAL-*K.pt` - Models trained against optimal minimax opponent
- `FINAL_MODEL.pt` - Best performing model after complete training pipeline

**Security Note**: All user input parsing has been secured and no longer uses `eval()`.

### Training and Experimentation
Training is done through Jupyter notebooks in the `experiments/` directories. Key notebooks:
- `models/ReinforcementLearning/DeepQ_TicTacToe_v2/experiments/train.ipynb` - **FULLY REPAIRED** training pipeline
  - âœ… Systematic two-phase training (NAIVE â†’ OPTIMAL opponents)
  - âœ… Comprehensive checkpoint evaluation and analysis
  - âœ… Supports both local and Google Colab execution
  - âœ… Reproducible experiments with seed control
  - âœ… Real-time progress tracking and visualization
  - âœ… Automatic model saving and validation

## Architecture Overview

### Core Components

**Reinforcement Learning (Primary Focus)** âœ… **FULLY REPAIRED**
- `DeepQAgent`: PyTorch DQN implementation with **FIXED**:
  - âœ… Device handling consistency (CPU/CUDA)
  - âœ… Memory management and race condition prevention
  - âœ… Epsilon decay logic conflicts resolved
  - âœ… Gradient clipping optimization
  - âœ… Target network synchronization timing
- `TicTacToeGame`: Game environment with **FIXED**:
  - âœ… Role switching and state synchronization
  - âœ… Minimax algorithm board corruption prevention
  - âœ… Turn validation and game logic consistency
  - âœ… Agent mode management (train/eval)
- `Utils.py`: Training utilities with **ENHANCED**:
  - âœ… Proper reward attribution and state transitions
  - âœ… Intelligent memory management
  - âœ… Comprehensive performance metrics
  - âœ… Statistical validation and confidence intervals

**Transformers**
- Custom from-scratch implementations (`Transformer.py`, `EncoderLayer.py`, `DecoderLayer.py`)
- HuggingFace integration wrapper (`GPT2Model.py`)

**Retrieval-Augmented Generation**
- ConceptNet API integration for external knowledge graphs
- KeyBERT-based concept extraction
- Knowledge representation utilities

### Project Structure

- `/models/` - Core AI implementations organized by domain
- `/trained_models/` - Saved model artifacts (.pt files, ONNX exports)
- `/experiments/` - Jupyter notebooks for training and evaluation
- `models/common.py` - Shared utilities (colors, path helpers)

### Key Design Patterns

**Opponent Types**: The RL system supports training against different opponent strategies:
- `NAIVE`: Random valid moves
- `OPTIMAL`: Minimax algorithm
- `AGENT`: Self-play or agent vs agent

**Model Versioning**: Two TicTacToe implementations (v1 and v2) with architectural improvements in v2

**Experimentation Workflow**: Comprehensive Jupyter notebook workflows for training, evaluation, and visualization

## Code Quality and Validation

### Environment Validation
```bash
# Validate Python environment and key dependencies
python -c "import torch, numpy, pandas, matplotlib; print(f'âœ“ Environment ready - PyTorch {torch.__version__}, NumPy {numpy.__version__}')"

# Check virtual environment is activated
python -c "import sys; print('âœ“ Virtual env active' if 'venv' in sys.prefix else 'âš ï¸  Virtual env not detected')"
```

### Model Validation
```bash
# Test model loading and basic functionality (UPDATED for fixed components)
python -c "
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL
import torch

# Test with proper parameter initialization
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
agent = DeepQAgent(
    device=device, epsilon=1.0, gamma=0.99, state_space=9, action_space=9,
    hidden_size=256, dropout=0.1, train_start=100, batch_size=32, memory_max_len=1000
)
game = TicTacToeGame(device, agent, OPPONENT_LEVEL.NAIVE, start_as_X=True)
print(f'âœ“ Core components load successfully on {device}')
print(f'âœ“ Agent version: {agent.__version__}')
print(f'âœ“ Game version: {game.__version__}')
"

# Validate trained model files
find trained_models/ -name "*.pt" -exec echo "âœ“ Found model: {}" \;

# Test device consistency and basic functionality
python -c "
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL
import torch

device = torch.device('cpu')
agent = DeepQAgent(device=device, epsilon=1.0, gamma=0.99, state_space=9, action_space=9)
game = TicTacToeGame(device, agent, OPPONENT_LEVEL.NAIVE)
state = game.get_state()
valid_moves, mask, indices = game.get_valid_moves()
action = agent.select_action(state, mask, indices)
print('âœ“ Device consistency and action selection working')
"

### Code Quality Checks
```bash
# Basic Python syntax validation (ALL FILES NOW PASS)
python -m py_compile models/ReinforcementLearning/DeepQ_TicTacToe_v2/*.py

# Check for common issues in Python files
python -c "
import ast
import glob
for file in glob.glob('models/**/*.py', recursive=True):
    try:
        with open(file, 'r') as f:
            ast.parse(f.read())
        print(f'âœ“ {file}')
    except SyntaxError as e:
        print(f'âœ— {file}: {e}')
"

# Validate fixes are working
python -c "
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame
from models.ReinforcementLearning.Utils import train_agent, test_agent
print('âœ“ All repaired components import successfully')
"

# Security validation (verify eval() removal)
grep -r "eval(" models/ReinforcementLearning/DeepQ_TicTacToe_v2/ || echo "âœ“ No unsafe eval() usage found"
```

### Development Workflow Validation
```bash
# Verify project structure
test -d models/ReinforcementLearning && echo "âœ“ Models directory exists"
test -d trained_models && echo "âœ“ Trained models directory exists"
test -f setup.py && echo "âœ“ Setup.py exists"

# Check if Jupyter notebooks are accessible
python -c "import jupyter; print('âœ“ Jupyter available')" 2>/dev/null || echo "âš ï¸  Jupyter not available"
```

## Troubleshooting and Best Practices

### ğŸ› Common Issues and Solutions

#### Device-Related Errors
```bash
# If you encounter CUDA/CPU device mismatches:
python -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA device count: {torch.cuda.device_count()}')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')
"
```

#### Training Failures
```bash
# Quick validation before training:
python -c "
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent
from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL
import torch

device = torch.device('cpu')  # Start with CPU for testing
agent = DeepQAgent(device=device, epsilon=1.0, gamma=0.99, state_space=9, action_space=9)
game = TicTacToeGame(device, agent, OPPONENT_LEVEL.NAIVE)

# Test basic functionality
state = game.get_state()
valid_moves, mask, indices = game.get_valid_moves()
action = agent.select_action(state, mask, indices)
next_state, reward, done = game.take_action(action)

print('âœ“ All basic operations work correctly')
print(f'State shape: {state.shape}, Action: {action}, Reward: {reward}')
"
```

### ğŸ“Š Training Best Practices

#### Recommended Training Sequence
1. **Baseline Validation**: Always test with untrained model first
2. **Phase 1**: Train against NAIVE opponent (50K episodes)
3. **Phase 2**: Train against OPTIMAL opponent (50K episodes)  
4. **Validation**: Test against both opponent types for performance metrics

#### Hyperparameter Guidelines
```python
# Recommended starting parameters for TicTacToe:
LEARNING_RATE = 0.001      # Adam optimizer works well
GAMMA = 0.99               # High discount for strategic planning
HIDDEN_SIZE = 256          # Sufficient for TicTacToe complexity
BATCH_SIZE = 64            # Good balance of stability/efficiency
MEMORY_MAX_LEN = 10000     # Adequate experience diversity
NETWORK_SYNC_RATE = 100    # Target network update frequency
```

#### Performance Benchmarks
- **Deployment Ready**: >80% win rate vs NAIVE opponent
- **Strategic Competence**: >30% win rate vs OPTIMAL opponent  
- **Training Stability**: Consistent loss decrease over episodes
- **Memory Efficiency**: Stable memory usage during long training

### ğŸ”’ Security Notes
- All user input in `play.py` is now safely parsed (no `eval()` usage)
- Model loading uses `weights_only=True` for security
- All file paths are validated before use
- Error messages don't expose sensitive system information

## Recent Major Improvements (December 2024)

### ğŸ”§ DQN Implementation Repairs
**All 32 identified issues have been systematically fixed:**
- âœ… **15 Critical Issues**: Device mismatches, memory races, security vulnerabilities
- âœ… **12 Major Issues**: Algorithm correctness, training stability, performance  
- âœ… **5 Minor Issues**: Code quality, readability, maintainability

### ğŸ“‹ Repair Documentation
- `DQN_TICTACTOE_V2_ISSUES.md` - Complete analysis of all identified problems
- `DQN_TICTACTOE_V2_REPAIR_CHECKLIST.md` - Systematic repair methodology and validation

### ğŸ§ª Validation Results
- âœ… All Python files pass syntax validation
- âœ… Core components instantiate without errors
- âœ… Training completes without crashes
- âœ… Device handling works correctly (CPU/CUDA)
- âœ… Security vulnerabilities eliminated
- âœ… Memory management stabilized

## Important Notes

- âœ… **FIXED**: `play` command now has proper error handling and secure input parsing
- âœ… **IMPROVED**: Training pipeline is now production-ready with comprehensive validation
- âœ… **ENHANCED**: All validation commands updated to reflect repaired components
- âœ… **SECURED**: All user input parsing secured (eval() usage eliminated)
- âœ… **OPTIMIZED**: Model paths use `get_root_directory()` helper for cross-platform compatibility
- âœ… **COMPREHENSIVE**: Training progress tracked with detailed logging and model checkpointing
- âœ… **AUTOMATED**: Custom commands in `.claude/commands/` provide shortcuts for common development tasks

### ğŸš€ Training Readiness
The codebase is now ready for reliable, high-quality agent training with:
- Expected >80% win rate vs NAIVE opponents (deployment ready)
- Expected >30% win rate vs OPTIMAL opponents (strategic competence)
- Stable loss convergence and consistent checkpoint improvements
- Comprehensive experiment tracking and model comparison tools

---

## ğŸ“š Quick Reference

### Essential Files
- `models/ReinforcementLearning/DeepQ_TicTacToe_v2/experiments/train.ipynb` - Main training notebook
- `DQN_TICTACTOE_V2_ISSUES.md` - Complete issue analysis  
- `DQN_TICTACTOE_V2_REPAIR_CHECKLIST.md` - Repair methodology and validation

### Key Commands
```bash
# Quick setup and validation
python3 -m venv .venv && source .venv/bin/activate && pip install -e .

# Test components
python -c "from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent; print('âœ“ Ready')"

# Play against trained model
python -m models.ReinforcementLearning.DeepQ_TicTacToe_v2.play FINAL_MODEL
```

### Performance Targets
- ğŸ¯ **NAIVE Opponent**: >80% win rate (deployment ready)
- ğŸ¯ **OPTIMAL Opponent**: >30% win rate (strategic competence)
- ğŸ¯ **Training Time**: ~2-4 hours for complete pipeline
- ğŸ¯ **Memory Usage**: <2GB RAM for training

---

*This documentation reflects the fully repaired and validated DQN implementation as of December 2024.*